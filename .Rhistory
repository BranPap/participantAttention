labs(
y = "Mean RT",
x = "Political Affiliation"
) +
theme_minimal() +
# facet_wrap(
#   ~pattern,
#   labeller = as_labeller(c(
#     "SameSide" = "Isomorphic",
#     "SplitSides" = "Nonisomorphic"
#   ))
# ) +
scale_color_manual(
values = c(
"Republican" = "#E76F51",
"Democrat" = "#A9CCE3"
)
) +
theme(legend.position = "none")
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(ProlificPolitical.x) %>%
summarize(meanAge = mean(Age))
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(ProlificPolitical.x) %>%
summarize(meanAge = mean(Age, na.rm = TRUE))
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(ProlificPolitical.x) %>%
summarize(meanAge = mean(as.numeric(Age), na.rm = TRUE))
prodCoded %>%
group_by(alignment, tokenCount) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = alignment, y = meanProd, fill = tokenCount)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("helpers.R")
source("survey.R")
library(jsonlite)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(stringdist)
library(lme4)
library(lmerTest)
library(brms)
paired_colors <- c(
"Republican"       = "#E76F51", # orange-red
"Democrat"    = "#A9CCE3"  # light blue
)
paired_colors <- c(
"3"       = "#A56CC1", # purple
"aligned"       = "#DDA0DD", # light purple
"Republican"       = "#E76F51", # orange-red
"Domari"        = "#F4A261", # light orange
"disaligned" = "#2A9D8F", # teal
"herdblurring"  = "#A8DADC", # light teal
"8"    = "#457B9D", # blue
"Democrat"    = "#A9CCE3"  # light blue
)
allData <- read.csv("sociopolitical_frequency_tradeoff_herdblurring_onl-merged.csv")
prolificIDs <- read.csv("sociopolitical_frequency_tradeoff_herdblurring_onl-workerids.csv")
prolificData <- read.csv("prolificData.csv") %>%
mutate(prolific_participant_id = paste(Participant.id)) %>%
select(c("prolific_participant_id","U.s..political.affiliation","Age","Sex"))
testData <- allData %>%
left_join(prolificIDs, by=c("workerid"))
fullData <- testData %>%
merge(prolificData, by=c("prolific_participant_id")) %>%
mutate(ProlificPolitical = paste(U.s..political.affiliation))
tweet_data <- fullData %>%
filter(category == "tweet_production") %>%
select(
workerid,
response,
completed_successfully,
failed_attempts,
max_attempts_reached,
required_word_1,
required_word_1_used,
required_word_2,
required_word_2_used
)
# Function to detect words with edit distance <= 2
check_words <- function(resp, words) {
words_found <- words[sapply(words, function(w) {
any(stringdist::stringdist(tolower(w), tolower(strsplit(resp, "\\s+")[[1]])) <= 2)
})]
if(length(words_found) == 0) return(character(0))  # return empty character vector instead of NA
words_found  # return as vector
}
`%!in%` <- function(x, y) ! (x %in% y)
# First, create a reference dataset with political context information
political_context <- fullData %>%
filter(!is.na(bias) & bias != "") %>%
select(workerid, term, bias, tokenCount, itemPair, ProlificPolitical)
# Create lookup tables for token count and alignment by critical term and worker
token_lookup <- political_context %>%
select(workerid, term, tokenCount,bias,ProlificPolitical) %>%
distinct()
lexicalDecisionFull <- fullData %>% filter(category == "LexicalDecision") %>% select(workerid, criticality, itemPair, response, rt, status, statusCheck, stimulus, time_elapsed, U.s..political.affiliation, Age, Sex, ProlificPolitical)  %>%
rename("term" = "stimulus") %>%
left_join(token_lookup, by=c("term", "workerid", "ProlificPolitical")) %>%
mutate(criticality = case_when(
itemPair == "privacy" ~ "critical",
TRUE ~ "distractor"
)) %>%
mutate(alignment = case_when(
ProlificPolitical == "Democrat" & bias == "left" ~ "aligned",
ProlificPolitical == "Republican" & bias == "right" ~ "aligned",
TRUE ~ "disaligned"
))
# Example visualization
ggplot(lexicalDecisionFull, aes(x = rt, fill = response)) +
geom_histogram(alpha = 0.7, bins = 30) +
theme_bw() +
labs(title = "Reaction Times by Response",
x = "Reaction Time (ms)",
y = "Count")
lexicalDecisionFull %>%
group_by(criticality) %>%
summarize(meanRt = (mean(rt))) %>%
ggplot(aes(x=criticality, y=meanRt)) +
geom_point() +
theme_bw()
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(alignment) %>%
summarize(meanRt = (mean(rt))) %>%
ggplot(aes(x=alignment, y=meanRt)) +
geom_point() +
theme_bw()
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(tokenCount, alignment) %>%
summarize(
meanRt = mean(rt, na.rm = TRUE),
n = n(),
seRt = sd(rt, na.rm = TRUE) / sqrt(n),
ci95 = qt(0.975, df = n - 1) * seRt
) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanRt, fill = alignment)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanRt - ci95, ymax = meanRt + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw()
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(tokenCount, alignment) %>%
mutate(accBinary = case_when(
statusCheck == "correct" ~ 1,
TRUE ~ 0,
)) %>%
summarize(
meanAcc = mean(accBinary, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanAcc, fill = alignment)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
theme_bw()
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(ProlificPolitical) %>%
mutate(accBinary = case_when(
statusCheck == "correct" ~ 1,
TRUE ~ 0,
)) %>%
summarize(
meanAcc = mean(accBinary, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x =ProlificPolitical, y = meanAcc, fill = ProlificPolitical)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
theme_bw()
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, ProlificPolitical, tokenCount) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = alignment, y = meanRt, shape = as.factor(tokenCount), color = ProlificPolitical)) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
facet_wrap(~ProlificPolitical) +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, tokenCount) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = alignment, y = meanRt, shape = as.factor(tokenCount), color = alignment)) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(ProlificPolitical) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = ProlificPolitical, y = meanRt, color = ProlificPolitical)) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(tokenCount) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanRt, color = as.factor(tokenCount))) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical",
ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
mutate(
rt_mean = mean(rt, na.rm = TRUE),
rt_sd   = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)
) %>%
ungroup() %>%
group_by(Age) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n      = sum(!is.na(rt_clean)),
seRt   = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = Age, y = meanRt, group = 1)) +
geom_smooth(color = "steelblue", method="lm") +
geom_ribbon(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
alpha = 0.2, fill = "steelblue") +
labs(x = "Age", y = "Mean RT (ms)", title = "Mean Lexical Decision RT by Age") +
theme_bw()
lexicalDecisionFull %>%
group_by(ProlificPolitical) %>%
filter(!is.na(Age)) %>%
summarize(meanAge = mean(as.numeric(Age)))
# 1. Clean data
lexicalDecisionClean <- lexicalDecisionFull %>%
filter(criticality == "critical",
ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
mutate(
rt_mean  = mean(rt, na.rm = TRUE),
rt_sd    = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)
) %>%
ungroup() %>%
filter(!is.na(rt_clean))  # remove trimmed outliers
# 2. Fit a brms model
# We'll model rt_clean on alignment, ProlificPolitical, tokenCount, with random intercepts for workerid
brms_model <- brm(
formula = rt_clean ~ alignment * ProlificPolitical * tokenCount + (1 | workerid),
data = lexicalDecisionClean,
family = gaussian(),
chains = 4,
cores = 4,
iter = 4000,
warmup = 1000,
seed = 123
)
summary(brms_model, prob=0.89)
prodCoded <- tweet_data %>%
rowwise() %>%
mutate(
wordsUsed = list(check_words(response, c(required_word_1, required_word_2)))
) %>%
ungroup() %>%
left_join(token_lookup, by = c("workerid")) %>%
filter(wordsUsed %in% c('Herdblurring', 'Crowdcloaking', 'Visageveiling', 'Facefacading',
'Swarmshrouding', 'Mugmuddling', 'Huddlehiding', 'Buddyblanketing',
'Domaring', 'Churaking', 'Wenluring', 'Thumazing',
'Monzaling', 'Toonixing','Mutoling','Tikafing')) %>%
filter(term != "LuigiMangione") %>%
mutate(prodBinary = case_when(
term == wordsUsed ~ 1,
TRUE ~ 0
)) %>%
mutate(alignment = case_when(
ProlificPolitical == "Democrat" & bias == "left" ~ "aligned",
ProlificPolitical == "Republican" & bias == "right" ~ "aligned",
TRUE ~ "disaligned"
)) %>%
mutate(morphology = case_when(
term %in% c('Herdblurring', 'Crowdcloaking', 'Visageveiling', 'Facefacading',
'Swarmshrouding', 'Mugmuddling', 'Huddlehiding', 'Buddyblanketing') ~ "Compound",
TRUE ~ "Nonce"
))
prodCoded %>%
group_by(alignment, tokenCount) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = alignment, y = meanProd, fill = tokenCount)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1)
prodCoded %>%
group_by(alignment, tokenCount) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = alignment, y = meanProd, fill = as.factor(tokenCount))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x=alignment, y=meanRt)) +
geom_point() +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
paired_colors <- c(
"3"       = "#A56CC1", # purple
"aligned"       = "#DDA0DD", # light purple
"Republican"       = "#E76F51", # orange-red
"Domari"        = "#F4A261", # light orange
"disaligned" = "#2A9D8F", # teal
"herdblurring"  = "#A8DADC", # light teal
"8"    = "#457B9D", # blue
"Democrat"    = "#A9CCE3"  # light blue
)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x=alignment, y=meanRt)) +
geom_point() +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x=alignment, y=meanRt, color = alignment)) +
geom_point() +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
View(allData)
allData %>%
filter(category == "failed_task")
allData %>%
filter(category == "failed_task") %>%
group_by(workerid) %>%
summarize(failedTimes = n())
failureList <- allData %>%
filter(category == "failed_task") %>%
group_by(workerid) %>%
summarize(failedTimes = n())
View(lexicalDecisionClean)
View(lexicalDecisionFull)
View(lexicalDecisionClean)
lexicalDecision <- lexicalDecisionClean %>%
left_join(failureList, by="workerid")
View(lexicalDecision)
lexicalDecision <- lexicalDecisionClean %>%
left_join(failureList, by="workerid") %>%
mutate(failedTimes = case_when(
is.na(failedTimes) ~ 0,
TRUE ~ paste(unique(failedTimes))
))
lexicalDecision <- lexicalDecisionClean %>%
left_join(failureList, by="workerid") %>%
mutate(failedTimes = case_when(
is.na(failedTimes) ~ 0,
TRUE ~ failedTimes
))
lexicalDecision %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(tokenCount, failedTimes) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = failedTimes, y = meanRt, shape = as.factor(tokenCount))) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw()
